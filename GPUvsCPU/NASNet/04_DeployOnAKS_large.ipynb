{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Web App on Azure Container Services (AKS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will set up an Azure Container Service which will be managed by Kubernetes. We will then take the Docker image we created earlier that contains our app and deploy it to the AKS cluster. Then, we will check everything is working by sending an image to it and getting it scored. \n",
    "\n",
    "The process is split into the following steps:\n",
    "- Define our resource names\n",
    "- Login to Azure\n",
    "- Create resource group and create AKS\n",
    "- Connect to AKS\n",
    "- Deploy our app\n",
    "- Tear it all down\n",
    "\n",
    "We assume that this notebook is running on Linux and Azure CLI is installed before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the various name definitions for the resources needed to setup AKS as well as the name of the Docker image we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please modify the below as you see fit\n",
    "resource_group = \"<RESOURCE_GROUP>\" \n",
    "aks_name = \"<AKS_CLUSTER_NAME>\"\n",
    "location = \"eastus\"\n",
    "\n",
    "image_name = '<YOUR_DOCKER_IMAGE>' # 'fboylu/kerastf-gpu' Feel free to use this image if you want to \n",
    "                                   # skip creating your own container\n",
    "selected_subscription = \"'<YOUR_SUBSCRIPTION>'\" # If you have multiple subscriptions select \n",
    "                                                # the subscription you want to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"fbaksnaslrg\" # Feel free to modify these\n",
    "aks_name = \"fbAKSnaslCluster\"\n",
    "location = \"eastus\"\n",
    "\n",
    "image_name = \"fboylu/kerasnasl-gpu\" \n",
    "selected_subscription = \"'Team Danielle Internal'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure account login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az account set --subscription $selected_subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"environmentName\": \"AzureCloud\",\r\n",
      "  \"id\": \"edf507a2-6235-46c5-b560-fd463ba2e771\",\r\n",
      "  \"isDefault\": true,\r\n",
      "  \"name\": \"Team Danielle Internal\",\r\n",
      "  \"state\": \"Enabled\",\r\n",
      "  \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\r\n",
      "  \"user\": {\r\n",
      "    \"name\": \"fboylu@microsoft.com\",\r\n",
      "    \"type\": \"user\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to register the container service resources on your subscription if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az provider register -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az provider show -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create resources and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create resource group and AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure encourages the use of groups to organize all the Azure components you deploy. That way it is easier to find them but also we can delete a number of resources simply by deleting the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/fbaksnaslrg\",\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"managedBy\": null,\r\n",
      "  \"name\": \"fbaksnaslrg\",\r\n",
      "  \"properties\": {\r\n",
      "    \"provisioningState\": \"Succeeded\"\r\n",
      "  },\r\n",
      "  \"tags\": null\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az group create --name $resource_group --location $location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create the AKS cluster in the resource group we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..principal creation[##################################]  100.0000%\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"count\": 1,\n",
      "      \"dnsPrefix\": null,\n",
      "      \"fqdn\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"osDiskSizeGb\": null,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"ports\": null,\n",
      "      \"storageProfile\": \"ManagedDisks\",\n",
      "      \"vmSize\": \"Standard_NC6\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"dnsPrefix\": \"fbAKSnaslC-fbaksnaslrg-edf507\",\n",
      "  \"fqdn\": \"fbaksnaslc-fbaksnaslrg-edf507-7218af32.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/fbaksnaslrg/providers/Microsoft.ContainerService/managedClusters/fbAKSnaslCluster\",\n",
      "  \"kubernetesVersion\": \"1.9.6\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTZYQFHNstYCR25qtvMrC6baTMS6TobaIRbgd0xOoafDy+2uBk0DMJuhGWoOcrsCnvadp5k/0K8qBRysyhlQGWb6+r8fBunThy+zpTKqdh3W8Q1y5UtKnGwwU1cqGXDOPUIXJYNPJqUKV829+MOrZjUynhHgSzDbY2ncGyoT+Farsvm01aGEdDapa+XRl4JAwtN1bb9q+Ii5y+MkpIOhLRMwATl05eNfAHmYQWtaIJZZJOHMNPswlBmLs293Wsj11vYh6/yo9S4ToEsc9Pbl5Zn6OFIu7jfzN2bM8cA3+8pru9WSthrxjJvPn8i4uTYozOdNIi09ArQ4lRT9t6rsMz\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"fbAKSnaslCluster\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"fbaksnaslrg\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"eeba3bfe-45f9-42de-9bdb-54416b67382d\",\n",
      "    \"keyVaultSecretRef\": null,\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0mCPU times: user 10.6 s, sys: 1.86 s, total: 12.5 s\n",
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az aks create --resource-group $resource_group --name $aks_name --node-count 1 --generate-ssh-keys -s Standard_NC6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name              Location    ResourceGroup    KubernetesVersion    ProvisioningState    Fqdn\r\n",
      "----------------  ----------  ---------------  -------------------  -------------------  -----------------------------------------------------------\r\n",
      "dangdevopsclstr   eastus      dangdevops       1.9.6                Succeeded            devops-3b040ce9.hcp.eastus.azmk8s.io\r\n",
      "ddaks             eastus      ddaksdeploy      1.8.11               Failed               ddaks-ddaksdeploy-edf507-eefe283f.hcp.eastus.azmk8s.io\r\n",
      "fbAKSClustercpu   eastus      fbakscpurg       1.8.10               Succeeded            fbaksclust-fbakscpurg-edf507-17685694.hcp.eastus.azmk8s.io\r\n",
      "fbAKSnaslCluster  eastus      fbaksnaslrg      1.9.6                Succeeded            fbaksnaslc-fbaksnaslrg-edf507-7218af32.hcp.eastus.azmk8s.io\r\n",
      "msaksgpu          eastus      msaksrg          1.9.6                Succeeded            msaksgpu-msaksrg-edf507-0eeb29a6.hcp.eastus.azmk8s.io\r\n"
     ]
    }
   ],
   "source": [
    "!az aks list -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install kubectl CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Kubernetes cluster, we will use kubectl, the Kubernetes command-line client. To install, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to /usr/local/bin/kubectl from https://storage.googleapis.com/kubernetes-release/release/v1.10.3/bin/linux/amd64/kubectl\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo -i az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure kubectl to connect to the Kubernetes cluster, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"fbAKSnaslCluster\" as current context in /home/fboylu/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group $resource_group --name $aks_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify connection by listing the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-nodepool1-55237306-0   Ready     agent     14m       v1.9.6\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the pods on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "kube-system   azureproxy-79c5db744-97j2l              1/1       Running   2          14m\r\n",
      "kube-system   heapster-55f855b47-h8zz5                2/2       Running   0          13m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-c9bzs           3/3       Running   0          14m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-nh8tf           3/3       Running   0          14m\r\n",
      "kube-system   kube-proxy-kcsss                        1/1       Running   0          14m\r\n",
      "kube-system   kube-svc-redirect-7vvc9                 1/1       Running   0          14m\r\n",
      "kube-system   kubernetes-dashboard-546f987686-rx47d   1/1       Running   1          14m\r\n",
      "kube-system   tunnelfront-fd46fb4b9-8ld49             1/1       Running   0          14m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define our Kubernetes manifest file for our service and load balancer. Note that we have to specify the volume mounts to the drivers that are located on the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_template = {\n",
    "  \"apiVersion\": \"apps/v1beta1\",\n",
    "  \"kind\": \"Deployment\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-dl\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"replicas\":1,\n",
    "      \"template\":{\n",
    "          \"metadata\":{\n",
    "              \"labels\":{\n",
    "                  \"app\":\"azure-dl\"\n",
    "              }\n",
    "          },\n",
    "          \"spec\":{\n",
    "              \"containers\":[\n",
    "                  {\n",
    "                      \"name\": \"azure-dl\",\n",
    "                      \"image\": \"fboylu/kerasnasl-gpu\",\n",
    "                      \"env\":[\n",
    "                          {\n",
    "                              \"name\": \"LD_LIBRARY_PATH\",\n",
    "                              \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.5/lib\"\n",
    "                          }\n",
    "                      ],\n",
    "                      \"ports\":[\n",
    "                          {\n",
    "                              \"containerPort\":80,\n",
    "                              \"name\":\"model\"\n",
    "                          }\n",
    "                      ],\n",
    "                      \"volumeMounts\":[\n",
    "                          {\n",
    "                              \"mountPath\":\"/usr/local/nvidia\",\n",
    "                              \"name\": \"nvidia\",\n",
    "                          }\n",
    "                      ],\n",
    "                      \"resources\":{\n",
    "                           \"requests\":{\n",
    "                               \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                           },\n",
    "                           \"limits\":{\n",
    "                               \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                           }\n",
    "                       }  \n",
    "                  }\n",
    "              ],\n",
    "              \"volumes\":[\n",
    "                  {\n",
    "                      \"name\": \"nvidia\",\n",
    "                      \"hostPath\":{\n",
    "                          \"path\":\"/usr/local/nvidia\"\n",
    "                      },\n",
    "                  },\n",
    "              ]\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "service_temp = {\n",
    "  \"apiVersion\": \"v1\",\n",
    "  \"kind\": \"Service\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-dl\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"type\": \"LoadBalancer\",\n",
    "      \"ports\":[\n",
    "          {\n",
    "              \"port\":80\n",
    "          }\n",
    "      ],\n",
    "      \"selector\":{\n",
    "            \"app\":\"azure-dl\"\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json_to_file(json_dict, filename, mode='w'):\n",
    "    with open(filename, mode) as outfile:\n",
    "        json.dump(json_dict, outfile, indent=4, sort_keys=True)\n",
    "        outfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(app_template, 'az-dl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(service_temp, 'az-dl.json', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the manifest created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"apiVersion\": \"apps/v1beta1\",\r\n",
      "    \"kind\": \"Deployment\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"replicas\": 1,\r\n",
      "        \"template\": {\r\n",
      "            \"metadata\": {\r\n",
      "                \"labels\": {\r\n",
      "                    \"app\": \"azure-dl\"\r\n",
      "                }\r\n",
      "            },\r\n",
      "            \"spec\": {\r\n",
      "                \"containers\": [\r\n",
      "                    {\r\n",
      "                        \"env\": [\r\n",
      "                            {\r\n",
      "                                \"name\": \"LD_LIBRARY_PATH\",\r\n",
      "                                \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.5/lib\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"image\": \"fboylu/kerasnasl-gpu\",\r\n",
      "                        \"name\": \"azure-dl\",\r\n",
      "                        \"ports\": [\r\n",
      "                            {\r\n",
      "                                \"containerPort\": 80,\r\n",
      "                                \"name\": \"model\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"resources\": {\r\n",
      "                            \"limits\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            },\r\n",
      "                            \"requests\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            }\r\n",
      "                        },\r\n",
      "                        \"volumeMounts\": [\r\n",
      "                            {\r\n",
      "                                \"mountPath\": \"/usr/local/nvidia\",\r\n",
      "                                \"name\": \"nvidia\"\r\n",
      "                            }\r\n",
      "                        ]\r\n",
      "                    }\r\n",
      "                ],\r\n",
      "                \"volumes\": [\r\n",
      "                    {\r\n",
      "                        \"hostPath\": {\r\n",
      "                            \"path\": \"/usr/local/nvidia\"\r\n",
      "                        },\r\n",
      "                        \"name\": \"nvidia\"\r\n",
      "                    }\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "    \"apiVersion\": \"v1\",\r\n",
      "    \"kind\": \"Service\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"ports\": [\r\n",
      "            {\r\n",
      "                \"port\": 80\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"selector\": {\r\n",
      "            \"app\": \"azure-dl\"\r\n",
      "        },\r\n",
      "        \"type\": \"LoadBalancer\"\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use kubectl create command to deploy our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"azure-dl\" created\n",
      "service \"azure-dl\" created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the pod is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "default       azure-dl-74c4d5d9d8-7fzvw               1/1       Running   0          10m\r\n",
      "kube-system   azureproxy-79c5db744-97j2l              1/1       Running   2          26m\r\n",
      "kube-system   heapster-55f855b47-h8zz5                2/2       Running   0          24m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-c9bzs           3/3       Running   0          26m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-nh8tf           3/3       Running   0          26m\r\n",
      "kube-system   kube-proxy-kcsss                        1/1       Running   0          26m\r\n",
      "kube-system   kube-svc-redirect-7vvc9                 1/1       Running   0          26m\r\n",
      "kube-system   kubernetes-dashboard-546f987686-rx47d   1/1       Running   1          26m\r\n",
      "kube-system   tunnelfront-fd46fb4b9-8ld49             1/1       Running   0          26m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything goes wrong you can use the commands below to observe the events on the node as well as review the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   FIRST SEEN   COUNT     NAME                                         KIND         SUBOBJECT                   TYPE      REASON                    SOURCE                                 MESSAGE\r\n",
      "28m         28m          1         aks-nodepool1-55237306-0.153559872ca2345b    Node                                     Normal    Starting                  kubelet, aks-nodepool1-55237306-0      Starting kubelet.\r\n",
      "27m         28m          8         aks-nodepool1-55237306-0.153559872ebdfb3c    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-55237306-0      Node aks-nodepool1-55237306-0 status is now: NodeHasSufficientDisk\r\n",
      "27m         28m          8         aks-nodepool1-55237306-0.153559872ebe5578    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-55237306-0      Node aks-nodepool1-55237306-0 status is now: NodeHasSufficientMemory\r\n",
      "27m         28m          7         aks-nodepool1-55237306-0.153559872ebe7e18    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-55237306-0      Node aks-nodepool1-55237306-0 status is now: NodeHasNoDiskPressure\r\n",
      "28m         28m          1         aks-nodepool1-55237306-0.153559872fb01da2    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-55237306-0      Updated Node Allocatable limit across pods\r\n",
      "26m         26m          1         aks-nodepool1-55237306-0.153559a33e5c1a4d    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-55237306-0 event: Registered Node aks-nodepool1-55237306-0 in Controller\r\n",
      "25m         25m          1         aks-nodepool1-55237306-0.153559a78af02e03    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-55237306-0   Starting kube-proxy.\r\n",
      "10m         10m          1         azure-dl-74c4d5d9d8-7fzvw.15355a7cb32978ed   Pod                                      Normal    Scheduled                 default-scheduler                      Successfully assigned azure-dl-74c4d5d9d8-7fzvw to aks-nodepool1-55237306-0\r\n",
      "10m         10m          1         azure-dl-74c4d5d9d8-7fzvw.15355a7cbf79ba53   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-55237306-0      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "10m         10m          1         azure-dl-74c4d5d9d8-7fzvw.15355a7cc0347b0c   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-55237306-0      MountVolume.SetUp succeeded for volume \"default-token-fmswn\" \r\n",
      "6m          10m          3         azure-dl-74c4d5d9d8-7fzvw.15355a7cf6c20d3b   Pod          spec.containers{azure-dl}   Normal    Pulling                   kubelet, aks-nodepool1-55237306-0      pulling image \"fboylu/kerasnasl-gpu\"\r\n",
      "7m          9m           2         azure-dl-74c4d5d9d8-7fzvw.15355a92be2eb09b   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-55237306-0      Failed to pull image \"fboylu/kerasnasl-gpu\": rpc error: code = Canceled desc = context canceled\r\n",
      "7m          9m           2         azure-dl-74c4d5d9d8-7fzvw.15355a92be2f14ff   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-55237306-0      Error: ErrImagePull\r\n",
      "6m          9m           2         azure-dl-74c4d5d9d8-7fzvw.15355a92d1bb77a9   Pod          spec.containers{azure-dl}   Normal    BackOff                   kubelet, aks-nodepool1-55237306-0      Back-off pulling image \"fboylu/kerasnasl-gpu\"\r\n",
      "6m          9m           2         azure-dl-74c4d5d9d8-7fzvw.15355a92d1bba5c1   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-55237306-0      Error: ImagePullBackOff\r\n",
      "3m          3m           1         azure-dl-74c4d5d9d8-7fzvw.15355ae539a517df   Pod          spec.containers{azure-dl}   Normal    Pulled                    kubelet, aks-nodepool1-55237306-0      Successfully pulled image \"fboylu/kerasnasl-gpu\"\r\n",
      "3m          3m           1         azure-dl-74c4d5d9d8-7fzvw.15355ae54a32b089   Pod          spec.containers{azure-dl}   Normal    Created                   kubelet, aks-nodepool1-55237306-0      Created container\r\n",
      "3m          3m           1         azure-dl-74c4d5d9d8-7fzvw.15355ae552bda985   Pod          spec.containers{azure-dl}   Normal    Started                   kubelet, aks-nodepool1-55237306-0      Started container\r\n",
      "10m         10m          1         azure-dl-74c4d5d9d8.15355a7cb1c39d36         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-74c4d5d9d8-7fzvw\r\n",
      "10m         10m          1         azure-dl.15355a7cb050bdbb                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled up replica set azure-dl-74c4d5d9d8 to 1\r\n",
      "10m         10m          1         azure-dl.15355a7cb552b8a8                    Service                                  Normal    EnsuringLoadBalancer      service-controller                     Ensuring load balancer\r\n",
      "7m          7m           1         azure-dl.15355aa5fbb68419                    Service                                  Normal    EnsuredLoadBalancer       service-controller                     Ensured load balancer\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs for the application pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_json = !kubectl get pods -o json\n",
    "pod_dict = json.loads(''.join(pod_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-05 19:39:49,581 CRIT Supervisor running as root (no user in config file)\r\n",
      "2018-06-05 19:39:49,583 INFO supervisord started with pid 1\r\n",
      "2018-06-05 19:39:50,584 INFO spawned: 'program_exit' with pid 9\r\n",
      "2018-06-05 19:39:50,586 INFO spawned: 'nginx' with pid 10\r\n",
      "2018-06-05 19:39:50,588 INFO spawned: 'gunicorn' with pid 11\r\n",
      "2018-06-05 19:39:51,618 INFO success: program_exit entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n",
      "2018-06-05 19:39:52.002225: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2018-06-05 19:39:52.233672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: e2e1:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\r\n",
      "2018-06-05 19:39:52.233718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n",
      "2018-06-05 19:39:52.540533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: e2e1:00:00.0, compute capability: 3.7)\r\n",
      "2018-06-05 19:39:56,542 INFO success: nginx entered RUNNING state, process has stayed up for > than 5 seconds (startsecs)\r\n",
      "2018-06-05 19:40:11,559 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 20 seconds (startsecs)\r\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/NASNet-large.h5\r\n",
      "\r\n",
      "     8192/359746192 [..............................] - ETA: 1:21\r\n",
      "   507904/359746192 [..............................] - ETA: 37s \r\n",
      "  2113536/359746192 [..............................] - ETA: 17s\r\n",
      "  5726208/359746192 [..............................] - ETA: 9s \r\n",
      " 11862016/359746192 [..............................] - ETA: 6s\r\n",
      " 14786560/359746192 [>.............................] - ETA: 6s\r\n",
      " 16039936/359746192 [>.............................] - ETA: 6s\r\n",
      " 24715264/359746192 [=>............................] - ETA: 4s\r\n",
      " 33292288/359746192 [=>............................] - ETA: 3s\r\n",
      " 42704896/359746192 [==>...........................] - ETA: 3s\r\n",
      " 52051968/359746192 [===>..........................] - ETA: 3s\r\n",
      " 58793984/359746192 [===>..........................] - ETA: 2s\r\n",
      " 67526656/359746192 [====>.........................] - ETA: 2s\r\n",
      " 70639616/359746192 [====>.........................] - ETA: 2s\r\n",
      " 72474624/359746192 [=====>........................] - ETA: 2s\r\n",
      " 74915840/359746192 [=====>........................] - ETA: 2s\r\n",
      " 82124800/359746192 [=====>........................] - ETA: 2s\r\n",
      " 89800704/359746192 [======>.......................] - ETA: 2s\r\n",
      " 92487680/359746192 [======>.......................] - ETA: 2s\r\n",
      "101367808/359746192 [=======>......................] - ETA: 2s\r\n",
      "109068288/359746192 [========>.....................] - ETA: 2s\r\n",
      "110379008/359746192 [========>.....................] - ETA: 2s\r\n",
      "120061952/359746192 [=========>....................] - ETA: 2s\r\n",
      "128688128/359746192 [=========>....................] - ETA: 2s\r\n",
      "134045696/359746192 [==========>...................] - ETA: 2s\r\n",
      "138092544/359746192 [==========>...................] - ETA: 2s\r\n",
      "144113664/359746192 [===========>..................] - ETA: 2s\r\n",
      "149053440/359746192 [===========>..................] - ETA: 1s\r\n",
      "150642688/359746192 [===========>..................] - ETA: 2s\r\n",
      "154550272/359746192 [===========>..................] - ETA: 2s\r\n",
      "160342016/359746192 [============>.................] - ETA: 1s\r\n",
      "166846464/359746192 [============>.................] - ETA: 1s\r\n",
      "174022656/359746192 [=============>................] - ETA: 1s\r\n",
      "178692096/359746192 [=============>................] - ETA: 1s\r\n",
      "185139200/359746192 [==============>...............] - ETA: 1s\r\n",
      "189112320/359746192 [==============>...............] - ETA: 1s\r\n",
      "191275008/359746192 [==============>...............] - ETA: 1s\r\n",
      "193241088/359746192 [===============>..............] - ETA: 1s\r\n",
      "198172672/359746192 [===============>..............] - ETA: 1s\r\n",
      "203857920/359746192 [================>.............] - ETA: 1s\r\n",
      "209625088/359746192 [================>.............] - ETA: 1s\r\n",
      "214654976/359746192 [================>.............] - ETA: 1s\r\n",
      "220995584/359746192 [=================>............] - ETA: 1s\r\n",
      "225828864/359746192 [=================>............] - ETA: 1s\r\n",
      "229883904/359746192 [==================>...........] - ETA: 1s\r\n",
      "232497152/359746192 [==================>...........] - ETA: 1s\r\n",
      "235331584/359746192 [==================>...........] - ETA: 1s\r\n",
      "240361472/359746192 [===================>..........] - ETA: 1s\r\n",
      "246521856/359746192 [===================>..........] - ETA: 1s\r\n",
      "252911616/359746192 [====================>.........] - ETA: 1s\r\n",
      "259530752/359746192 [====================>.........] - ETA: 1s\r\n",
      "263905280/359746192 [=====================>........] - ETA: 0s\r\n",
      "269950976/359746192 [=====================>........] - ETA: 0s\r\n",
      "272113664/359746192 [=====================>........] - ETA: 0s\r\n",
      "277250048/359746192 [======================>.......] - ETA: 0s\r\n",
      "283369472/359746192 [======================>.......] - ETA: 0s\r\n",
      "290537472/359746192 [=======================>......] - ETA: 0s\r\n",
      "296706048/359746192 [=======================>......] - ETA: 0s\r\n",
      "302440448/359746192 [========================>.....] - ETA: 0s\r\n",
      "308502528/359746192 [========================>.....] - ETA: 0s\r\n",
      "313843712/359746192 [=========================>....] - ETA: 0s\r\n",
      "314990592/359746192 [=========================>....] - ETA: 0s\r\n",
      "322363392/359746192 [=========================>....] - ETA: 0s\r\n",
      "330080256/359746192 [==========================>...] - ETA: 0s\r\n",
      "336863232/359746192 [===========================>..] - ETA: 0s\r\n",
      "343072768/359746192 [===========================>..] - ETA: 0s\r\n",
      "350232576/359746192 [============================>.] - ETA: 0s\r\n",
      "357588992/359746192 [============================>.] - ETA: 0s\r\n",
      "359751680/359746192 [==============================] - 4s 0us/step\r\n",
      "{\"message\": \"Model loading time: 54135.64 ms\", \"level\": \"INFO\", \"stack_info\": null, \"host\": \"azure-dl-74c4d5d9d8-7fzvw\", \"logger\": \"model_driver\", \"path\": \"/code/driver.py\", \"tags\": [], \"timestamp\": \"2018-06-05T19:40:46.111822Z\"}\r\n",
      "Initialising\r\n",
      "{\"message\": \" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\", \"level\": \"INFO\", \"msg\": \" * Running on %s://%s:%d/ %s\", \"stack_info\": null, \"host\": \"azure-dl-74c4d5d9d8-7fzvw\", \"logger\": \"werkzeug\", \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"tags\": [], \"timestamp\": \"2018-06-05T19:40:46.117505Z\"}\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs {pod_dict['items'][0]['metadata']['name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "azure-dl   1         1         1            1           11m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a few minutes for the service to populate the EXTERNAL-IP field below. This will be the IP you use to call the service. You can also specify an IP to use, please see the AKS documentation for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\r\n",
      "azure-dl   LoadBalancer   10.0.251.102   137.117.94.64   80:32742/TCP   11m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service azure-dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will [test our web application deployed on AKS](05_TestWebApp.ipynb). Once, we are done with all the notebooks of the tutorial, below instructions can be used to delete the cluster and free resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"count\": 3,\n",
      "      \"dnsPrefix\": null,\n",
      "      \"fqdn\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"osDiskSizeGb\": null,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"ports\": null,\n",
      "      \"storageProfile\": \"ManagedDisks\",\n",
      "      \"vmSize\": \"Standard_NC6\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"dnsPrefix\": \"fbAKSnaslC-fbaksnaslrg-edf507\",\n",
      "  \"fqdn\": \"fbaksnaslc-fbaksnaslrg-edf507-7218af32.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/fbaksnaslrg/providers/Microsoft.ContainerService/managedClusters/fbAKSnaslCluster\",\n",
      "  \"kubernetesVersion\": \"1.9.6\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTZYQFHNstYCR25qtvMrC6baTMS6TobaIRbgd0xOoafDy+2uBk0DMJuhGWoOcrsCnvadp5k/0K8qBRysyhlQGWb6+r8fBunThy+zpTKqdh3W8Q1y5UtKnGwwU1cqGXDOPUIXJYNPJqUKV829+MOrZjUynhHgSzDbY2ncGyoT+Farsvm01aGEdDapa+XRl4JAwtN1bb9q+Ii5y+MkpIOhLRMwATl05eNfAHmYQWtaIJZZJOHMNPswlBmLs293Wsj11vYh6/yo9S4ToEsc9Pbl5Zn6OFIu7jfzN2bM8cA3+8pru9WSthrxjJvPn8i4uTYozOdNIi09ArQ4lRT9t6rsMz\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"fbAKSnaslCluster\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"fbaksnaslrg\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"eeba3bfe-45f9-42de-9bdb-54416b67382d\",\n",
      "    \"keyVaultSecretRef\": null,\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az aks scale --resource-group=$resource_group --name=$aks_name --node-count 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               aks-nodepool1-55237306-0\n",
      "Roles:              agent\n",
      "Labels:             agentpool=nodepool1\n",
      "                    beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/instance-type=Standard_NC6\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    failure-domain.beta.kubernetes.io/region=eastus\n",
      "                    failure-domain.beta.kubernetes.io/zone=0\n",
      "                    kubernetes.azure.com/cluster=MC_fbaksnaslrg_fbAKSnaslCluster_eastus\n",
      "                    kubernetes.io/hostname=aks-nodepool1-55237306-0\n",
      "                    kubernetes.io/role=agent\n",
      "                    storageprofile=managed\n",
      "                    storagetier=Standard_LRS\n",
      "Annotations:        node.alpha.kubernetes.io/ttl=0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach=true\n",
      "CreationTimestamp:  Tue, 05 Jun 2018 19:16:15 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Conditions:\n",
      "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----                 ------  -----------------                 ------------------                ------                       -------\n",
      "  NetworkUnavailable   False   Tue, 05 Jun 2018 19:16:57 +0000   Tue, 05 Jun 2018 19:16:57 +0000   RouteCreated                 RouteController created a route\n",
      "  OutOfDisk            False   Wed, 06 Jun 2018 15:32:53 +0000   Tue, 05 Jun 2018 19:16:15 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n",
      "  MemoryPressure       False   Wed, 06 Jun 2018 15:32:53 +0000   Wed, 06 Jun 2018 15:16:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure         False   Wed, 06 Jun 2018 15:32:53 +0000   Wed, 06 Jun 2018 15:16:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  Ready                True    Wed, 06 Jun 2018 15:32:53 +0000   Wed, 06 Jun 2018 15:16:41 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\n",
      "Addresses:\n",
      "  InternalIP:  10.240.0.4\n",
      "  Hostname:    aks-nodepool1-55237306-0\n",
      "Capacity:\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\n",
      " cpu:                             6\n",
      " memory:                          57691676Ki\n",
      " pods:                            110\n",
      "Allocatable:\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\n",
      " cpu:                             6\n",
      " memory:                          57589276Ki\n",
      " pods:                            110\n",
      "System Info:\n",
      " Machine ID:                 19f95be8b7604fbd9e0b76325ed12be9\n",
      " System UUID:                0FDDABD2-F1F9-2444-A3E5-976C96DBC92B\n",
      " Boot ID:                    3f0582dc-a7d1-47f0-8b2a-be40d5ba9909\n",
      " Kernel Version:             4.13.0-1016-azure\n",
      " OS Image:                   Ubuntu 16.04.4 LTS\n",
      " Operating System:           linux\n",
      " Architecture:               amd64\n",
      " Container Runtime Version:  docker://1.13.1\n",
      " Kubelet Version:            v1.9.6\n",
      " Kube-Proxy Version:         v1.9.6\n",
      "PodCIDR:                     10.244.0.0/24\n",
      "ExternalID:                  /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-0\n",
      "ProviderID:                  azure:///subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-0\n",
      "Non-terminated Pods:         (9 in total)\n",
      "  Namespace                  Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits\n",
      "  ---------                  ----                                     ------------  ----------  ---------------  -------------\n",
      "  default                    azure-dl-74c4d5d9d8-7fzvw                0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                azureproxy-79c5db744-97j2l               0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                heapster-55f855b47-h8zz5                 138m (2%)     138m (2%)   294Mi (0%)       294Mi (0%)\n",
      "  kube-system                kube-dns-v20-7c556f89c5-c9bzs            110m (1%)     0 (0%)      120Mi (0%)       220Mi (0%)\n",
      "  kube-system                kube-dns-v20-7c556f89c5-nh8tf            110m (1%)     0 (0%)      120Mi (0%)       220Mi (0%)\n",
      "  kube-system                kube-proxy-kcsss                         100m (1%)     0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                kube-svc-redirect-7vvc9                  0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                kubernetes-dashboard-546f987686-rx47d    100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)\n",
      "  kube-system                tunnelfront-fd46fb4b9-8ld49              0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  CPU Requests  CPU Limits  Memory Requests  Memory Limits\n",
      "  ------------  ----------  ---------------  -------------\n",
      "  558m (9%)     238m (3%)   584Mi (1%)       784Mi (1%)\n",
      "Events:\n",
      "  Type    Reason                   Age                 From                               Message\n",
      "  ----    ------                   ----                ----                               -------\n",
      "  Normal  NodeHasSufficientMemory  16m (x10 over 20h)  kubelet, aks-nodepool1-55237306-0  Node aks-nodepool1-55237306-0 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    16m (x10 over 20h)  kubelet, aks-nodepool1-55237306-0  Node aks-nodepool1-55237306-0 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeReady                16m (x2 over 20h)   kubelet, aks-nodepool1-55237306-0  Node aks-nodepool1-55237306-0 status is now: NodeReady\n",
      "\n",
      "\n",
      "Name:               aks-nodepool1-55237306-1\n",
      "Roles:              agent\n",
      "Labels:             agentpool=nodepool1\n",
      "                    beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/instance-type=Standard_NC6\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    failure-domain.beta.kubernetes.io/region=eastus\n",
      "                    failure-domain.beta.kubernetes.io/zone=1\n",
      "                    kubernetes.azure.com/cluster=MC_fbaksnaslrg_fbAKSnaslCluster_eastus\n",
      "                    kubernetes.io/hostname=aks-nodepool1-55237306-1\n",
      "                    kubernetes.io/role=agent\n",
      "                    storageprofile=managed\n",
      "                    storagetier=Standard_LRS\n",
      "Annotations:        node.alpha.kubernetes.io/ttl=0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach=true\n",
      "CreationTimestamp:  Wed, 06 Jun 2018 13:28:38 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Conditions:\n",
      "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----                 ------  -----------------                 ------------------                ------                       -------\n",
      "  NetworkUnavailable   False   Wed, 06 Jun 2018 13:28:51 +0000   Wed, 06 Jun 2018 13:28:51 +0000   RouteCreated                 RouteController created a route\n",
      "  OutOfDisk            False   Wed, 06 Jun 2018 15:32:55 +0000   Wed, 06 Jun 2018 13:28:38 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n",
      "  MemoryPressure       False   Wed, 06 Jun 2018 15:32:55 +0000   Wed, 06 Jun 2018 13:28:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure         False   Wed, 06 Jun 2018 15:32:55 +0000   Wed, 06 Jun 2018 13:28:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  Ready                True    Wed, 06 Jun 2018 15:32:55 +0000   Wed, 06 Jun 2018 13:28:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\n",
      "Addresses:\n",
      "  InternalIP:  10.240.0.5\n",
      "  Hostname:    aks-nodepool1-55237306-1\n",
      "Capacity:\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\n",
      " cpu:                             6\n",
      " memory:                          57691676Ki\n",
      " pods:                            110\n",
      "Allocatable:\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\n",
      " cpu:                             6\n",
      " memory:                          57589276Ki\n",
      " pods:                            110\n",
      "System Info:\n",
      " Machine ID:                 a9270fa2a58e45b2b6a156c4f55d0047\n",
      " System UUID:                50F264E5-4CDD-974B-9DCC-30565D0D2DAA\n",
      " Boot ID:                    f526dfaf-32cd-4342-b4c3-4caf71e34719\n",
      " Kernel Version:             4.13.0-1016-azure\n",
      " OS Image:                   Ubuntu 16.04.4 LTS\n",
      " Operating System:           linux\n",
      " Architecture:               amd64\n",
      " Container Runtime Version:  docker://1.13.1\n",
      " Kubelet Version:            v1.9.6\n",
      " Kube-Proxy Version:         v1.9.6\n",
      "PodCIDR:                     10.244.1.0/24\n",
      "ExternalID:                  /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-1\n",
      "ProviderID:                  azure:///subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-1\n",
      "Non-terminated Pods:         (3 in total)\n",
      "  Namespace                  Name                         CPU Requests  CPU Limits  Memory Requests  Memory Limits\n",
      "  ---------                  ----                         ------------  ----------  ---------------  -------------\n",
      "  default                    azure-dl-74c4d5d9d8-mns89    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                kube-proxy-wpzpw             100m (1%)     0 (0%)      0 (0%)           0 (0%)\n",
      "  kube-system                kube-svc-redirect-vpqrn      0 (0%)        0 (0%)      0 (0%)           0 (0%)\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  CPU Requests  CPU Limits  Memory Requests  Memory Limits\n",
      "  ------------  ----------  ---------------  -------------\n",
      "  100m (1%)     0 (0%)      0 (0%)           0 (0%)\n",
      "Events:         <none>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "Name:               aks-nodepool1-55237306-2\r\n",
      "Roles:              agent\r\n",
      "Labels:             agentpool=nodepool1\r\n",
      "                    beta.kubernetes.io/arch=amd64\r\n",
      "                    beta.kubernetes.io/instance-type=Standard_NC6\r\n",
      "                    beta.kubernetes.io/os=linux\r\n",
      "                    failure-domain.beta.kubernetes.io/region=eastus\r\n",
      "                    failure-domain.beta.kubernetes.io/zone=0\r\n",
      "                    kubernetes.azure.com/cluster=MC_fbaksnaslrg_fbAKSnaslCluster_eastus\r\n",
      "                    kubernetes.io/hostname=aks-nodepool1-55237306-2\r\n",
      "                    kubernetes.io/role=agent\r\n",
      "                    storageprofile=managed\r\n",
      "                    storagetier=Standard_LRS\r\n",
      "Annotations:        node.alpha.kubernetes.io/ttl=0\r\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach=true\r\n",
      "CreationTimestamp:  Wed, 06 Jun 2018 15:21:42 +0000\r\n",
      "Taints:             <none>\r\n",
      "Unschedulable:      false\r\n",
      "Conditions:\r\n",
      "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\r\n",
      "  ----                 ------  -----------------                 ------------------                ------                       -------\r\n",
      "  NetworkUnavailable   False   Wed, 06 Jun 2018 15:22:05 +0000   Wed, 06 Jun 2018 15:22:05 +0000   RouteCreated                 RouteController created a route\r\n",
      "  OutOfDisk            False   Wed, 06 Jun 2018 15:32:54 +0000   Wed, 06 Jun 2018 15:21:42 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\r\n",
      "  MemoryPressure       False   Wed, 06 Jun 2018 15:32:54 +0000   Wed, 06 Jun 2018 15:21:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\r\n",
      "  DiskPressure         False   Wed, 06 Jun 2018 15:32:54 +0000   Wed, 06 Jun 2018 15:21:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\r\n",
      "  Ready                True    Wed, 06 Jun 2018 15:32:54 +0000   Wed, 06 Jun 2018 15:22:02 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\r\n",
      "Addresses:\r\n",
      "  InternalIP:  10.240.0.6\r\n",
      "  Hostname:    aks-nodepool1-55237306-2\r\n",
      "Capacity:\r\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\r\n",
      " cpu:                             6\r\n",
      " memory:                          57691676Ki\r\n",
      " pods:                            110\r\n",
      "Allocatable:\r\n",
      " alpha.kubernetes.io/nvidia-gpu:  1\r\n",
      " cpu:                             6\r\n",
      " memory:                          57589276Ki\r\n",
      " pods:                            110\r\n",
      "System Info:\r\n",
      " Machine ID:                 d99c192a6506401388f94c856d62a7b3\r\n",
      " System UUID:                E2B15636-82F7-2747-B996-2341A8FF8616\r\n",
      " Boot ID:                    10e3e6c5-d0be-40a2-842a-05de63aa82bf\r\n",
      " Kernel Version:             4.13.0-1016-azure\r\n",
      " OS Image:                   Ubuntu 16.04.4 LTS\r\n",
      " Operating System:           linux\r\n",
      " Architecture:               amd64\r\n",
      " Container Runtime Version:  docker://1.13.1\r\n",
      " Kubelet Version:            v1.9.6\r\n",
      " Kube-Proxy Version:         v1.9.6\r\n",
      "PodCIDR:                     10.244.2.0/24\r\n",
      "ExternalID:                  /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-2\r\n",
      "ProviderID:                  azure:///subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbaksnaslrg_fbAKSnaslCluster_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-55237306-2\r\n",
      "Non-terminated Pods:         (2 in total)\r\n",
      "  Namespace                  Name                       CPU Requests  CPU Limits  Memory Requests  Memory Limits\r\n",
      "  ---------                  ----                       ------------  ----------  ---------------  -------------\r\n",
      "  kube-system                kube-proxy-zlhhs           100m (1%)     0 (0%)      0 (0%)           0 (0%)\r\n",
      "  kube-system                kube-svc-redirect-7g4kf    0 (0%)        0 (0%)      0 (0%)           0 (0%)\r\n",
      "Allocated resources:\r\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\r\n",
      "  CPU Requests  CPU Limits  Memory Requests  Memory Limits\r\n",
      "  ------------  ----------  ---------------  -------------\r\n",
      "  100m (1%)     0 (0%)      0 (0%)           0 (0%)\r\n",
      "Events:\r\n",
      "  Type    Reason                   Age                From                                  Message\r\n",
      "  ----    ------                   ----               ----                                  -------\r\n",
      "  Normal  Starting                 11m                kubelet, aks-nodepool1-55237306-2     Starting kubelet.\r\n",
      "  Normal  NodeHasSufficientDisk    11m (x2 over 11m)  kubelet, aks-nodepool1-55237306-2     Node aks-nodepool1-55237306-2 status is now: NodeHasSufficientDisk\r\n",
      "  Normal  NodeHasSufficientMemory  11m (x2 over 11m)  kubelet, aks-nodepool1-55237306-2     Node aks-nodepool1-55237306-2 status is now: NodeHasSufficientMemory\r\n",
      "  Normal  NodeHasNoDiskPressure    11m (x2 over 11m)  kubelet, aks-nodepool1-55237306-2     Node aks-nodepool1-55237306-2 status is now: NodeHasNoDiskPressure\r\n",
      "  Normal  NodeAllocatableEnforced  11m                kubelet, aks-nodepool1-55237306-2     Updated Node Allocatable limit across pods\r\n",
      "  Normal  NodeReady                10m                kubelet, aks-nodepool1-55237306-2     Node aks-nodepool1-55237306-2 status is now: NodeReady\r\n",
      "  Normal  Starting                 10m                kube-proxy, aks-nodepool1-55237306-2  Starting kube-proxy.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions \"azure-dl\" scaled\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl scale --current-replicas=2 --replicas=3 deployment/azure-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "default       azure-dl-74c4d5d9d8-7fzvw               1/1       Running   0          21h\r\n",
      "default       azure-dl-74c4d5d9d8-mns89               1/1       Running   0          3h\r\n",
      "default       azure-dl-74c4d5d9d8-s4qbf               1/1       Running   0          1h\r\n",
      "kube-system   azureproxy-79c5db744-97j2l              1/1       Running   2          22h\r\n",
      "kube-system   heapster-55f855b47-h8zz5                2/2       Running   0          22h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-c9bzs           3/3       Running   0          22h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-nh8tf           3/3       Running   0          22h\r\n",
      "kube-system   kube-proxy-kcsss                        1/1       Running   0          22h\r\n",
      "kube-system   kube-proxy-wpzpw                        1/1       Running   0          3h\r\n",
      "kube-system   kube-proxy-zlhhs                        1/1       Running   0          2h\r\n",
      "kube-system   kube-svc-redirect-7g4kf                 1/1       Running   0          2h\r\n",
      "kube-system   kube-svc-redirect-7vvc9                 1/1       Running   0          22h\r\n",
      "kube-system   kube-svc-redirect-vpqrn                 1/1       Running   0          3h\r\n",
      "kube-system   kubernetes-dashboard-546f987686-rx47d   1/1       Running   1          22h\r\n",
      "kube-system   tunnelfront-fd46fb4b9-8ld49             1/1       Running   0          22h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear it all down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with your cluster you can use the following two commands to destroy it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"azure-dl\" deleted\n",
      "service \"azure-dl\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f az-dl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az aks delete -n $aks_name -g $resource_group -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az group delete --name $resource_group -y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aksenv]",
   "language": "python",
   "name": "conda-env-aksenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
