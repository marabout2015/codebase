{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Web App on Azure Container Services (AKS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will set up an Azure Container Service which will be managed by Kubernetes. We will then take the Docker image we created earlier that contains our app and deploy it to the AKS cluster. Then, we will check everything is working by sending an image to it and getting it scored. \n",
    "\n",
    "The process is split into the following steps:\n",
    "- Define our resource names\n",
    "- Login to Azure\n",
    "- Create resource group and create AKS\n",
    "- Connect to AKS\n",
    "- Deploy our app\n",
    "- Tear it all down\n",
    "\n",
    "We assume that this notebook is running on Linux and Azure CLI is installed before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the various name definitions for the resources needed to setup AKS as well as the name of the Docker image we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please modify the below as you see fit\n",
    "resource_group = \"<RESOURCE_GROUP>\" \n",
    "aks_name = \"<AKS_CLUSTER_NAME>\"\n",
    "location = \"eastus\"\n",
    "\n",
    "image_name = '<YOUR_DOCKER_IMAGE>' # 'fboylu/kerastf-gpu' Feel free to use this image if you want to \n",
    "                                   # skip creating your own container\n",
    "selected_subscription = \"'<YOUR_SUBSCRIPTION>'\" # If you have multiple subscriptions select \n",
    "                                                # the subscription you want to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"fbaksrg\" # Feel free to modify these\n",
    "aks_name = \"fbAKSClustergpu\"\n",
    "location = \"eastus\"\n",
    "\n",
    "image_name = \"fboylu/kerasres50-gpu\" \n",
    "selected_subscription = \"'Team Danielle Internal'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure account login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az account set --subscription $selected_subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"environmentName\": \"AzureCloud\",\r\n",
      "  \"id\": \"edf507a2-6235-46c5-b560-fd463ba2e771\",\r\n",
      "  \"isDefault\": true,\r\n",
      "  \"name\": \"Team Danielle Internal\",\r\n",
      "  \"state\": \"Enabled\",\r\n",
      "  \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\r\n",
      "  \"user\": {\r\n",
      "    \"name\": \"fboylu@microsoft.com\",\r\n",
      "    \"type\": \"user\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to register the container service resources on your subscription if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az provider register -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!az provider show -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create resources and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create resource group and AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure encourages the use of groups to organize all the Azure components you deploy. That way it is easier to find them but also we can delete a number of resources simply by deleting the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/fbaksrg\",\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"managedBy\": null,\r\n",
      "  \"name\": \"fbaksrg\",\r\n",
      "  \"properties\": {\r\n",
      "    \"provisioningState\": \"Succeeded\"\r\n",
      "  },\r\n",
      "  \"tags\": null\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az group create --name $resource_group --location $location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create the AKS cluster in the resource group we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"count\": 1,\n",
      "      \"dnsPrefix\": null,\n",
      "      \"fqdn\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"osDiskSizeGb\": null,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"ports\": null,\n",
      "      \"storageProfile\": \"ManagedDisks\",\n",
      "      \"vmSize\": \"Standard_NC6\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"dnsPrefix\": \"fbAKSClust-fbaksrg-edf507\",\n",
      "  \"fqdn\": \"fbaksclust-fbaksrg-edf507-e12d7f40.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/fbaksrg/providers/Microsoft.ContainerService/managedClusters/fbAKSClustergpu\",\n",
      "  \"kubernetesVersion\": \"1.9.6\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTZYQFHNstYCR25qtvMrC6baTMS6TobaIRbgd0xOoafDy+2uBk0DMJuhGWoOcrsCnvadp5k/0K8qBRysyhlQGWb6+r8fBunThy+zpTKqdh3W8Q1y5UtKnGwwU1cqGXDOPUIXJYNPJqUKV829+MOrZjUynhHgSzDbY2ncGyoT+Farsvm01aGEdDapa+XRl4JAwtN1bb9q+Ii5y+MkpIOhLRMwATl05eNfAHmYQWtaIJZZJOHMNPswlBmLs293Wsj11vYh6/yo9S4ToEsc9Pbl5Zn6OFIu7jfzN2bM8cA3+8pru9WSthrxjJvPn8i4uTYozOdNIi09ArQ4lRT9t6rsMz\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"fbAKSClustergpu\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"fbaksrg\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"eeba3bfe-45f9-42de-9bdb-54416b67382d\",\n",
      "    \"keyVaultSecretRef\": null,\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0mCPU times: user 15.4 s, sys: 5.92 s, total: 21.3 s\n",
      "Wall time: 15min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az aks create --resource-group $resource_group --name $aks_name --node-count 1 --generate-ssh-keys -s Standard_NC6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install kubectl CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Kubernetes cluster, we will use kubectl, the Kubernetes command-line client. To install, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to /usr/local/bin/kubectl from https://storage.googleapis.com/kubernetes-release/release/v1.10.4/bin/linux/amd64/kubectl\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure kubectl to connect to the Kubernetes cluster, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"fbAKSClustergpu\" as current context in /home/fboylu/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group $resource_group --name $aks_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify connection by listing the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-nodepool1-28016997-0   Ready     agent     25m       v1.9.6\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the pods on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "kube-system   azureproxy-79c5db744-r5ggd              1/1       Running   2          29m\r\n",
      "kube-system   heapster-55f855b47-4m7xr                2/2       Running   0          24m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-4z4z6           3/3       Running   0          29m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-mp5fh           3/3       Running   0          29m\r\n",
      "kube-system   kube-proxy-k8t2c                        1/1       Running   0          26m\r\n",
      "kube-system   kube-svc-redirect-z6ppp                 1/1       Running   0          26m\r\n",
      "kube-system   kubernetes-dashboard-546f987686-8krxm   1/1       Running   2          29m\r\n",
      "kube-system   tunnelfront-695bcbdc68-t4l8t            1/1       Running   0          29m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define our Kubernetes manifest file for our service and load balancer. Note that we have to specify the volume mounts to the drivers that are located on the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_template = {\n",
    "  \"apiVersion\": \"apps/v1beta1\",\n",
    "  \"kind\": \"Deployment\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-dl\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"replicas\":1,\n",
    "      \"template\":{\n",
    "          \"metadata\":{\n",
    "              \"labels\":{\n",
    "                  \"app\":\"azure-dl\"\n",
    "              }\n",
    "          },\n",
    "          \"spec\":{\n",
    "              \"containers\":[\n",
    "                  {\n",
    "                      \"name\": \"azure-dl\",\n",
    "                      \"image\": \"fboylu/kerasres50-gpu\",\n",
    "                      \"env\":[\n",
    "                          {\n",
    "                              \"name\": \"LD_LIBRARY_PATH\",\n",
    "                              \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.5/lib\"\n",
    "                          }\n",
    "                      ],\n",
    "                      \"ports\":[\n",
    "                          {\n",
    "                              \"containerPort\":80,\n",
    "                              \"name\":\"model\"\n",
    "                          }\n",
    "                      ],\n",
    "                      \"volumeMounts\":[\n",
    "                          {\n",
    "                              \"mountPath\":\"/usr/local/nvidia\",\n",
    "                              \"name\": \"nvidia\",\n",
    "                          }\n",
    "                      ],\n",
    "                      \"resources\":{\n",
    "                           \"requests\":{\n",
    "                               \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                           },\n",
    "                           \"limits\":{\n",
    "                               \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                           }\n",
    "                       }  \n",
    "                  }\n",
    "              ],\n",
    "              \"volumes\":[\n",
    "                  {\n",
    "                      \"name\": \"nvidia\",\n",
    "                      \"hostPath\":{\n",
    "                          \"path\":\"/usr/local/nvidia\"\n",
    "                      },\n",
    "                  },\n",
    "              ]\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "service_temp = {\n",
    "  \"apiVersion\": \"v1\",\n",
    "  \"kind\": \"Service\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-dl\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"type\": \"LoadBalancer\",\n",
    "      \"ports\":[\n",
    "          {\n",
    "              \"port\":80\n",
    "          }\n",
    "      ],\n",
    "      \"selector\":{\n",
    "            \"app\":\"azure-dl\"\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json_to_file(json_dict, filename, mode='w'):\n",
    "    with open(filename, mode) as outfile:\n",
    "        json.dump(json_dict, outfile, indent=4, sort_keys=True)\n",
    "        outfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(app_template, 'az-dl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(service_temp, 'az-dl.json', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the manifest created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"apiVersion\": \"apps/v1beta1\",\r\n",
      "    \"kind\": \"Deployment\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"replicas\": 1,\r\n",
      "        \"template\": {\r\n",
      "            \"metadata\": {\r\n",
      "                \"labels\": {\r\n",
      "                    \"app\": \"azure-dl\"\r\n",
      "                }\r\n",
      "            },\r\n",
      "            \"spec\": {\r\n",
      "                \"containers\": [\r\n",
      "                    {\r\n",
      "                        \"env\": [\r\n",
      "                            {\r\n",
      "                                \"name\": \"LD_LIBRARY_PATH\",\r\n",
      "                                \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.5/lib\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"image\": \"fboylu/kerasres50-gpu\",\r\n",
      "                        \"name\": \"azure-dl\",\r\n",
      "                        \"ports\": [\r\n",
      "                            {\r\n",
      "                                \"containerPort\": 80,\r\n",
      "                                \"name\": \"model\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"resources\": {\r\n",
      "                            \"limits\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            },\r\n",
      "                            \"requests\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            }\r\n",
      "                        },\r\n",
      "                        \"volumeMounts\": [\r\n",
      "                            {\r\n",
      "                                \"mountPath\": \"/usr/local/nvidia\",\r\n",
      "                                \"name\": \"nvidia\"\r\n",
      "                            }\r\n",
      "                        ]\r\n",
      "                    }\r\n",
      "                ],\r\n",
      "                \"volumes\": [\r\n",
      "                    {\r\n",
      "                        \"hostPath\": {\r\n",
      "                            \"path\": \"/usr/local/nvidia\"\r\n",
      "                        },\r\n",
      "                        \"name\": \"nvidia\"\r\n",
      "                    }\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "    \"apiVersion\": \"v1\",\r\n",
      "    \"kind\": \"Service\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"ports\": [\r\n",
      "            {\r\n",
      "                \"port\": 80\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"selector\": {\r\n",
      "            \"app\": \"azure-dl\"\r\n",
      "        },\r\n",
      "        \"type\": \"LoadBalancer\"\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use kubectl create command to deploy our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"azure-dl\" created\n",
      "service \"azure-dl\" created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the pod is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "default       azure-dl-7677f788fc-ld67s               1/1       Running   0          21m\r\n",
      "kube-system   azureproxy-79c5db744-r5ggd              1/1       Running   2          51m\r\n",
      "kube-system   heapster-55f855b47-4m7xr                2/2       Running   0          46m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-4z4z6           3/3       Running   0          51m\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-mp5fh           3/3       Running   0          51m\r\n",
      "kube-system   kube-proxy-k8t2c                        1/1       Running   0          48m\r\n",
      "kube-system   kube-svc-redirect-z6ppp                 1/1       Running   0          48m\r\n",
      "kube-system   kubernetes-dashboard-546f987686-8krxm   1/1       Running   2          51m\r\n",
      "kube-system   tunnelfront-695bcbdc68-t4l8t            1/1       Running   0          51m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything goes wrong you can use the commands below to observe the events on the node as well as review the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   FIRST SEEN   COUNT     NAME                                         KIND         SUBOBJECT                   TYPE      REASON                    SOURCE                                 MESSAGE\r\n",
      "36m         36m          8         aks-nodepool1-28016997-0.153632538a78ad4f    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-28016997-0      Node aks-nodepool1-28016997-0 status is now: NodeHasSufficientDisk\r\n",
      "36m         36m          8         aks-nodepool1-28016997-0.153632538a78dfb4    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-28016997-0      Node aks-nodepool1-28016997-0 status is now: NodeHasSufficientMemory\r\n",
      "36m         36m          8         aks-nodepool1-28016997-0.153632538a790598    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-28016997-0      Node aks-nodepool1-28016997-0 status is now: NodeHasNoDiskPressure\r\n",
      "36m         36m          1         aks-nodepool1-28016997-0.153632538b964e81    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-28016997-0      Updated Node Allocatable limit across pods\r\n",
      "31m         31m          1         aks-nodepool1-28016997-0.1536329a4522ec27    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-28016997-0 event: Registered Node aks-nodepool1-28016997-0 in Controller\r\n",
      "31m         31m          1         aks-nodepool1-28016997-0.1536329b9c397a2f    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-28016997-0   Starting kube-proxy.\r\n",
      "5m          11m          22        azure-dl-7677f788fc-4qnv7.153633b637bcafd2   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-4qnv7.15363409639734e9   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-4qnv7\r\n",
      "5m          11m          22        azure-dl-7677f788fc-52r9n.153633b632b5df62   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-52r9n.1536340963b7efbb   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-52r9n\r\n",
      "5m          11m          22        azure-dl-7677f788fc-5dhj4.153633b639c9c452   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-5dhj4.1536340963c7c7bd   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-5dhj4\r\n",
      "5m          11m          22        azure-dl-7677f788fc-6kjj5.153633b636a47fd5   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-6kjj5.153634096170944c   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-6kjj5\r\n",
      "5m          11m          22        azure-dl-7677f788fc-7w2dt.153633b6307110be   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-7w2dt.1536340963c71efc   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-7w2dt\r\n",
      "5m          11m          22        azure-dl-7677f788fc-7xj9f.153633b63bbf4189   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-7xj9f.153634096397c69e   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-7xj9f\r\n",
      "5m          11m          22        azure-dl-7677f788fc-brbj6.153633b631582b0b   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-brbj6.15363409613eeed9   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-brbj6\r\n",
      "5m          11m          22        azure-dl-7677f788fc-cdfmr.153633b633bdb1c4   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-cdfmr.1536340963c4ad92   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-cdfmr\r\n",
      "5m          11m          22        azure-dl-7677f788fc-cqrqz.153633b63926f4af   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-cqrqz.1536340963c65312   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-cqrqz\r\n",
      "5m          11m          22        azure-dl-7677f788fc-fbxpj.153633b6372189fa   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-fbxpj.15363409616ed7c0   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-fbxpj\r\n",
      "5m          11m          22        azure-dl-7677f788fc-g8h4z.153633b6384c25c3   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-g8h4z.1536340963c4f5d7   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-g8h4z\r\n",
      "5m          11m          22        azure-dl-7677f788fc-gksqf.153633b632341a41   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-gksqf.1536340963c6e463   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-gksqf\r\n",
      "5m          11m          22        azure-dl-7677f788fc-glb4d.153633b638e7a128   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-glb4d.153634096171acc7   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-glb4d\r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.1536341318b46d30   Pod                                      Normal    Scheduled                 default-scheduler                      Successfully assigned azure-dl-7677f788fc-ld67s to aks-nodepool1-28016997-0\r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.153634131e928583   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-28016997-0      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.153634131ffd53b0   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-28016997-0      MountVolume.SetUp succeeded for volume \"default-token-crgnj\" \r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.153634135566c746   Pod          spec.containers{azure-dl}   Normal    Pulling                   kubelet, aks-nodepool1-28016997-0      pulling image \"fboylu/kerasres50-gpu\"\r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.153634149005705b   Pod          spec.containers{azure-dl}   Normal    Pulled                    kubelet, aks-nodepool1-28016997-0      Successfully pulled image \"fboylu/kerasres50-gpu\"\r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.15363414aed48c90   Pod          spec.containers{azure-dl}   Normal    Created                   kubelet, aks-nodepool1-28016997-0      Created container\r\n",
      "4m          4m           1         azure-dl-7677f788fc-ld67s.15363414b87fbe09   Pod          spec.containers{azure-dl}   Normal    Started                   kubelet, aks-nodepool1-28016997-0      Started container\r\n",
      "5m          11m          22        azure-dl-7677f788fc-mdwwb.153633b63487ceb4   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-mdwwb.15363409639d8b3c   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-mdwwb\r\n",
      "5m          11m          22        azure-dl-7677f788fc-shxn7.153633b635273ea3   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-shxn7.1536340962fc72ab   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-shxn7\r\n",
      "5m          11m          22        azure-dl-7677f788fc-sz8fd.153633b63b161297   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-sz8fd.1536340963c83e7e   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-sz8fd\r\n",
      "5m          11m          22        azure-dl-7677f788fc-vk4sg.153633b63abe4b76   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-vk4sg.1536340963c5e1c9   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-vk4sg\r\n",
      "5m          11m          22        azure-dl-7677f788fc-vt7fw.153633b63c23ceb4   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-vt7fw.15363409616d048b   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-vt7fw\r\n",
      "22m         22m          1         azure-dl-7677f788fc-xc2m8.1536331db1d47bed   Pod                                      Normal    Scheduled                 default-scheduler                      Successfully assigned azure-dl-7677f788fc-xc2m8 to aks-nodepool1-28016997-0\r\n",
      "22m         22m          1         azure-dl-7677f788fc-xc2m8.1536331dbd774eef   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-28016997-0      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "22m         22m          1         azure-dl-7677f788fc-xc2m8.1536331dbe4512e8   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-28016997-0      MountVolume.SetUp succeeded for volume \"default-token-crgnj\" \r\n",
      "16m         22m          3         azure-dl-7677f788fc-xc2m8.1536331defb97a2a   Pod          spec.containers{azure-dl}   Normal    Pulling                   kubelet, aks-nodepool1-28016997-0      pulling image \"fboylu/kerasres50-gpu\"\r\n",
      "17m         20m          2         azure-dl-7677f788fc-xc2m8.1536332ea1702312   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-28016997-0      Failed to pull image \"fboylu/kerasres50-gpu\": rpc error: code = Canceled desc = context canceled\r\n",
      "17m         20m          2         azure-dl-7677f788fc-xc2m8.1536332ea1707132   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-28016997-0      Error: ErrImagePull\r\n",
      "17m         20m          2         azure-dl-7677f788fc-xc2m8.1536332ede426c5c   Pod          spec.containers{azure-dl}   Normal    BackOff                   kubelet, aks-nodepool1-28016997-0      Back-off pulling image \"fboylu/kerasres50-gpu\"\r\n",
      "17m         20m          2         azure-dl-7677f788fc-xc2m8.1536332ede429434   Pod          spec.containers{azure-dl}   Warning   Failed                    kubelet, aks-nodepool1-28016997-0      Error: ImagePullBackOff\r\n",
      "14m         14m          1         azure-dl-7677f788fc-xc2m8.15363382925ac307   Pod          spec.containers{azure-dl}   Normal    Pulled                    kubelet, aks-nodepool1-28016997-0      Successfully pulled image \"fboylu/kerasres50-gpu\"\r\n",
      "14m         14m          1         azure-dl-7677f788fc-xc2m8.15363382a213256b   Pod          spec.containers{azure-dl}   Normal    Created                   kubelet, aks-nodepool1-28016997-0      Created container\r\n",
      "14m         14m          1         azure-dl-7677f788fc-xc2m8.15363382a9954ac6   Pod          spec.containers{azure-dl}   Normal    Started                   kubelet, aks-nodepool1-28016997-0      Started container\r\n",
      "4m          4m           1         azure-dl-7677f788fc-xc2m8.1536340c07d93975   Pod          spec.containers{azure-dl}   Normal    Killing                   kubelet, aks-nodepool1-28016997-0      Killing container with id docker://azure-dl:Need to kill Pod\r\n",
      "5m          11m          22        azure-dl-7677f788fc-z8scn.153633b63a1ccd57   Pod                                      Warning   FailedScheduling          default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "5m          5m           1         azure-dl-7677f788fc-z8scn.1536340963c80aee   Pod                                      Warning   FailedScheduling          default-scheduler                      skip schedule deleting pod: default/azure-dl-7677f788fc-z8scn\r\n",
      "22m         22m          1         azure-dl-7677f788fc.1536331db0ed7e35         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-xc2m8\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b63066b25f         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-7w2dt\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b63135d494         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-gksqf\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b63150a9e0         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-brbj6\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b6322fd5a4         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-mdwwb\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b6323366f2         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-52r9n\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b63237d3c8         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-cdfmr\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b632567cca         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-shxn7\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b632bc6d16         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-fbxpj\r\n",
      "11m         11m          1         azure-dl-7677f788fc.153633b632bca9a2         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-6kjj5\r\n",
      "11m         11m          10        azure-dl-7677f788fc.153633b63ad7c4f4         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  (combined from similar events): Created pod: azure-dl-7677f788fc-7xj9f\r\n",
      "5m          5m           1         azure-dl-7677f788fc.15363409638f57a6         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-brbj6\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340963a118ac         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-xc2m8\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340963ec8ac9         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-6kjj5\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340963ee850c         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-fbxpj\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340963ef566d         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-vt7fw\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340963f115b4         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-glb4d\r\n",
      "5m          5m           1         azure-dl-7677f788fc.1536340964d726bf         ReplicaSet                               Normal    SuccessfulDelete          replicaset-controller                  Deleted pod: azure-dl-7677f788fc-shxn7\r\n",
      "4m          4m           1         azure-dl-7677f788fc.15363413184191d9         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-dl-7677f788fc-ld67s\r\n",
      "22m         22m          1         azure-dl.1536331daed82f46                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled up replica set azure-dl-7677f788fc to 1\r\n",
      "22m         22m          1         azure-dl.1536331db1be419d                    Service                                  Normal    EnsuringLoadBalancer      service-controller                     Ensuring load balancer\r\n",
      "18m         18m          1         azure-dl.153633495653e23e                    Service                                  Normal    EnsuredLoadBalancer       service-controller                     Ensured load balancer\r\n",
      "11m         11m          1         azure-dl.153633b62dbd705b                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled up replica set azure-dl-7677f788fc to 20\r\n",
      "5m          5m           1         azure-dl.1536340960f03069                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled down replica set azure-dl-7677f788fc to 0\r\n",
      "5m          5m           1         azure-dl.15363409b48f8ece                    Service                                  Normal    DeletingLoadBalancer      service-controller                     Deleting load balancer\r\n",
      "4m          4m           1         azure-dl.1536341317ba9c3c                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled up replica set azure-dl-7677f788fc to 1\r\n",
      "2m          2m           1         azure-dl.1536342fbdd8f957                    Service                                  Normal    DeletedLoadBalancer       service-controller                     Deleted load balancer\r\n",
      "2m          2m           1         azure-dl.1536342fbdd96653                    Service                                  Normal    EnsuringLoadBalancer      service-controller                     Ensuring load balancer\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs for the application pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_json = !kubectl get pods -o json\n",
    "pod_dict = json.loads(''.join(pod_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-08 13:59:47,185 CRIT Supervisor running as root (no user in config file)\r\n",
      "2018-06-08 13:59:47,188 INFO supervisord started with pid 1\r\n",
      "2018-06-08 13:59:48,190 INFO spawned: 'program_exit' with pid 10\r\n",
      "2018-06-08 13:59:48,192 INFO spawned: 'nginx' with pid 11\r\n",
      "2018-06-08 13:59:48,193 INFO spawned: 'gunicorn' with pid 12\r\n",
      "2018-06-08 13:59:49,226 INFO success: program_exit entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n",
      "2018-06-08 13:59:49.604948: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2018-06-08 13:59:49.796415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: ddde:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\r\n",
      "2018-06-08 13:59:49.796461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n",
      "2018-06-08 13:59:50.117612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: ddde:00:00.0, compute capability: 3.7)\r\n",
      "2018-06-08 13:59:54,122 INFO success: nginx entered RUNNING state, process has stayed up for > than 5 seconds (startsecs)\r\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\r\n",
      "\r\n",
      "     8192/102853048 [..............................] - ETA: 28s\r\n",
      "   663552/102853048 [..............................] - ETA: 8s \r\n",
      "  1851392/102853048 [..............................] - ETA: 5s\r\n",
      "  3620864/102853048 [>.............................] - ETA: 4s\r\n",
      "  5275648/102853048 [>.............................] - ETA: 3s\r\n",
      "  5521408/102853048 [>.............................] - ETA: 4s\r\n",
      "  6848512/102853048 [>.............................] - ETA: 4s\r\n",
      "  7561216/102853048 [=>............................] - ETA: 4s\r\n",
      "  7757824/102853048 [=>............................] - ETA: 6s\r\n",
      "  9502720/102853048 [=>............................] - ETA: 5s\r\n",
      " 11075584/102853048 [==>...........................] - ETA: 6s\r\n",
      " 12836864/102853048 [==>...........................] - ETA: 5s\r\n",
      " 14909440/102853048 [===>..........................] - ETA: 4s\r\n",
      " 16826368/102853048 [===>..........................] - ETA: 4s\r\n",
      " 18702336/102853048 [====>.........................] - ETA: 4s\r\n",
      " 20512768/102853048 [====>.........................] - ETA: 4s\r\n",
      " 22724608/102853048 [=====>........................] - ETA: 3s\r\n",
      " 24584192/102853048 [======>.......................] - ETA: 3s\r\n",
      " 26796032/102853048 [======>.......................] - ETA: 3s\r\n",
      " 29007872/102853048 [=======>......................] - ETA: 3s\r\n",
      " 30867456/102853048 [========>.....................] - ETA: 2s\r\n",
      " 32579584/102853048 [========>.....................] - ETA: 2s\r\n",
      " 34299904/102853048 [=========>....................] - ETA: 2s\r\n",
      " 35725312/102853048 [=========>....................] - ETA: 2s\r\n",
      " 37240832/102853048 [=========>....................] - ETA: 2s\r\n",
      " 38739968/102853048 [==========>...................] - ETA: 2s\r\n",
      " 40181760/102853048 [==========>...................] - ETA: 2s\r\n",
      " 41263104/102853048 [===========>..................] - ETA: 2s\r\n",
      " 42704896/102853048 [===========>..................] - ETA: 2s\r\n",
      " 44154880/102853048 [===========>..................] - ETA: 2s\r\n",
      " 45490176/102853048 [============>.................] - ETA: 2s\r\n",
      " 47063040/102853048 [============>.................] - ETA: 2s\r\n",
      " 48889856/102853048 [=============>................] - ETA: 2s\r\n",
      " 50888704/102853048 [=============>................] - ETA: 1s\r\n",
      " 52617216/102853048 [==============>...............] - ETA: 1s\r\n",
      " 53190656/102853048 [==============>...............] - ETA: 1s\r\n",
      " 54247424/102853048 [==============>...............] - ETA: 1s\r\n",
      " 55517184/102853048 [===============>..............] - ETA: 1s\r\n",
      " 56680448/102853048 [===============>..............] - ETA: 1s\r\n",
      " 57278464/102853048 [===============>..............] - ETA: 1s\r\n",
      " 59056128/102853048 [================>.............] - ETA: 1s\r\n",
      " 61489152/102853048 [================>.............] - ETA: 1s\r\n",
      " 63995904/102853048 [=================>............] - ETA: 1s\r\n",
      " 66625536/102853048 [==================>...........] - ETA: 1s\r\n",
      " 68263936/102853048 [==================>...........] - ETA: 1s\r\n",
      " 69615616/102853048 [===================>..........] - ETA: 1s\r\n",
      " 70344704/102853048 [===================>..........] - ETA: 1s\r\n",
      " 71262208/102853048 [===================>..........] - ETA: 1s\r\n",
      " 73940992/102853048 [====================>.........] - ETA: 1s\r\n",
      " 76292096/102853048 [=====================>........] - ETA: 0s\r\n",
      " 78635008/102853048 [=====================>........] - ETA: 0s\r\n",
      " 78970880/102853048 [======================>.......] - ETA: 0s\r\n",
      " 81084416/102853048 [======================>.......] - ETA: 0s\r\n",
      " 81559552/102853048 [======================>.......] - ETA: 0s\r\n",
      " 83574784/102853048 [=======================>......] - ETA: 0s\r\n",
      " 85590016/102853048 [=======================>......] - ETA: 0s\r\n",
      " 87662592/102853048 [========================>.....] - ETA: 0s\r\n",
      " 89800704/102853048 [=========================>....] - ETA: 0s\r\n",
      " 92102656/102853048 [=========================>....] - ETA: 0s\r\n",
      " 94257152/102853048 [==========================>...] - ETA: 0s\r\n",
      " 94879744/102853048 [==========================>...] - ETA: 0s\r\n",
      " 96518144/102853048 [===========================>..] - ETA: 0s\r\n",
      " 97714176/102853048 [===========================>..] - ETA: 0s\r\n",
      " 98721792/102853048 [===========================>..] - ETA: 0s\r\n",
      " 99975168/102853048 [============================>.] - ETA: 0s\r\n",
      "101195776/102853048 [============================>.] - ETA: 0s\r\n",
      "102621184/102853048 [============================>.] - ETA: 0s\r\n",
      "102858752/102853048 [==============================] - 4s 0us/step\r\n",
      "{\"tags\": [], \"stack_info\": null, \"message\": \"Model loading time: 12164.39 ms\", \"level\": \"INFO\", \"path\": \"/code/driver.py\", \"host\": \"azure-dl-7677f788fc-ld67s\", \"timestamp\": \"2018-06-08T14:00:01.737978Z\", \"logger\": \"model_driver\"}\r\n",
      "Initialising\r\n",
      "{\"tags\": [], \"stack_info\": null, \"message\": \" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\", \"level\": \"INFO\", \"msg\": \" * Running on %s://%s:%d/ %s\", \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"host\": \"azure-dl-7677f788fc-ld67s\", \"timestamp\": \"2018-06-08T14:00:01.744609Z\", \"logger\": \"werkzeug\"}\r\n",
      "2018-06-08 14:00:08,752 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 20 seconds (startsecs)\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs {pod_dict['items'][0]['metadata']['name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "azure-dl   1         1         1            1           4m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a few minutes for the service to populate the EXTERNAL-IP field below. This will be the IP you use to call the service. You can also specify an IP to use, please see the AKS documentation for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE\r\n",
      "azure-dl   LoadBalancer   10.0.150.121   40.121.69.40   80:31965/TCP   18m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service azure-dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will [test our web application deployed on AKS](05_TestWebApp.ipynb). Once, we are done with all the notebooks of the tutorial, below instructions can be used to delete the cluster and free resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az aks scale --resource-group=$resource_group --name=$aks_name --node-count 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl scale --current-replicas=1 --replicas=2 deployment/azure-dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear it all down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with your cluster you can use the following two commands to destroy it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"azure-dl\" deleted\n",
      "service \"azure-dl\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f az-dl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[K - Starting ..\r",
      "\r",
      "\u001b[K - Finished ..\r",
      "\r",
      "\u001b[K\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az aks delete -n $aks_name -g $resource_group -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K - Finished ..\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az group delete --name $resource_group -y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
